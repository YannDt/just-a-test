{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e5fbb8-6470-454b-b8c1-84870a8c058a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T14:23:37.764946Z",
     "iopub.status.busy": "2022-11-01T14:23:37.764712Z",
     "iopub.status.idle": "2022-11-01T14:24:14.437294Z",
     "shell.execute_reply": "2022-11-01T14:24:14.436642Z",
     "shell.execute_reply.started": "2022-11-01T14:23:37.764919Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dc234f8a884ef480aee0b4829a8b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1667311984141_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-6.ec2.internal:20888/proxy/application_1667311984141_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-332HO0R4T05XO\n",
       "\" application-id=\"application_1667311984141_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-8.ec2.internal:8042/node/containerlogs/container_1667311984141_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "      .builder\n",
    "      .config(\"spark.sql.broadcastTimeout\", \"36000\")\n",
    "      .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46665fa1-e772-4029-ad5e-b2ec23198a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T15:42:12.775592Z",
     "iopub.status.busy": "2022-11-01T15:42:12.775348Z",
     "iopub.status.idle": "2022-11-01T15:42:12.835811Z",
     "shell.execute_reply": "2022-11-01T15:42:12.835236Z",
     "shell.execute_reply.started": "2022-11-01T15:42:12.775565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a3a2e65ec343f89bdf3b2e7116b047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_p = \"https://s3.amazonaws.com/amazon-reviews-pds/parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4d1d7d6-ca5d-4435-9b49-f15cc9cb8bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-01T15:42:17.013811Z",
     "iopub.status.busy": "2022-11-01T15:42:17.013579Z",
     "iopub.status.idle": "2022-11-01T15:42:17.285336Z",
     "shell.execute_reply": "2022-11-01T15:42:17.284608Z",
     "shell.execute_reply.started": "2022-11-01T15:42:17.013788Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d429c30a6b84cdb8a06c1cd953a82ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o572.parquet.\n",
      ": java.lang.UnsupportedOperationException\n",
      "\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(InMemoryFileIndex.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$1.apply(InMemoryFileIndex.scala:202)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$1.apply(InMemoryFileIndex.scala:201)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.bulkListLeafFiles(InMemoryFileIndex.scala:160)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:147)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:72)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$createInMemoryFileIndex(DataSource.scala:547)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:385)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:667)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 316, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o572.parquet.\n",
      ": java.lang.UnsupportedOperationException\n",
      "\tat org.apache.hadoop.fs.http.AbstractHttpFileSystem.listStatus(AbstractHttpFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.http.HttpsFileSystem.listStatus(HttpsFileSystem.java:23)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(InMemoryFileIndex.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$1.apply(InMemoryFileIndex.scala:202)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$1.apply(InMemoryFileIndex.scala:201)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.bulkListLeafFiles(InMemoryFileIndex.scala:160)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:147)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:96)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:72)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$createInMemoryFileIndex(DataSource.scala:547)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:385)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:667)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.parquet(reviews_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
